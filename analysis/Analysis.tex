\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}

\setcounter{tocdepth}{3}

\begin{document}

\title{Analysis}
\date{}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Sets}
\begin{itemize}
\item \textbf{Definition:} A set is a collection of objects, called the \emph{elements} or \emph{members} of the set.  
We write $x \in X$ if $x$ is an element of the set $X$ and $x \notin X$ if $x$ is not an element of $X$.

Two sets $X = Y$, if
\[
x \in X \iff x \in Y
\]
(``iff'' or ``$\Leftrightarrow$'' both mean ``if and only if'').

The empty set is denoted by $\varnothing$, that is, the set without any elements.  
$X$ is \emph{nonempty} if it has at least one element.

We can define sets by listing their elements:
\[
X = \{a, b, c, d\}.
\]

We can also have infinite sets, for example:
\[
\mathbb{N} = \{1, 2, 3, \dots\}, \quad
\mathbb{N}_0 = \{0, 1, 2, 3, \dots\},
\]
\[
\mathbb{Z} = \{\dots, -3, -2, -1, 0, 1, 2, 3, \dots\}, \quad
\mathbb{Q} = \left\{ \tfrac{p}{q} : p, q \in \mathbb{Z}, \; q \neq 0 \right\},
\]
\[
\mathbb{R} = \{\text{all numbers with decimal expansions}\}.
\]
\end{itemize}
\subsection{Subsets}

$A$ is a subset of a set $X$ or $A$ is included in $X$, written $A \subseteq X$, if every element of $A$ belongs to $X$. $A$ is a proper subset of $X$, written as $A \subset X$, when $A \subseteq X$, but $A \neq X$.

\begin{itemize}
\item\textbf{Def.:} The power set $\mathcal{P}$($X$) of a set $X$ is the set of all subsets of $X$.

\item\textbf{Ex.:} $X=$\{1,2,3\}, then $\mathcal{P}$($X$)=\{$\varnothing$,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\}
\end{itemize}
The power set $\mathcal{P}(X)$ of a set $X$ with $|X|=n$ elements has $|\mathcal{P}(X)|=2^n$ elements because, in defining a subset, we have two independent choices for each element (does it belong to the subset or not?). Thus, the notation $2^X=\mathcal{P}(X)$ is also in use.

\subsection{Set operations}

The intersection $A\cap B$ of two sets $A$, $B$ is the set of all elements that belong to both $A$ and $B$. Two sets $A$, $B$ are said to be disjoint if $A\cap B = \varnothing $; that is, if $A$ and $B$ have no elements in common.

The union $A \cup B$ is the set of all elements that belong to $A$ or $B$. Note that we always use ‘or’ in an inclusive sense, so that $x \in A \cup B$ if $x$ is an element of $A$ or $B$, or both $A$ and $B$. (Thus, $A \cap B \subseteq A \cup B$.)

The set-difference of two sets $B$ and $A$ is the set of elements of $B$ that do not belong to $A$, that is $B \setminus A = \{x \in B:x \notin A\}$.
If we consider sets that are subsets of a fixed set $X$ (called the universe) that is understood from the context, then we write $A^c = \overline{A} = X \setminus A$ to denote the complement of $A \subseteq X$ in $X$. Note that $(A^c)^c = A$.

The Cartesian product $A \times B$ of sets $A$, $B$ is the set whose members all possible ordered pairs $(a, b)$ with $a\in A$, $b\in B$, thus $A\times B = \{(a,b):a\in A, b\in B \}$ and $|A\times B| = |A||B|$.

\subsection{Algebraic properties}

Intersection is a commutative operation $A\cap B = B \cap A$; and an \textit{associative} operation, that is:
\[
(A\cap B)\cap C = A \cap (B\cap C)\text{, thus} = A\cap B \cap C
\]
   both are also true for the union $A\cup B$.
Intersection distributes over union and union distributes over intersection:
\[
A\cap (B\cup C) = (A\cap B)\cup (A\cap C)
\]
\[
A\cup (B\cap C) = (A\cup B) \cap (A\cup C)
\]
We have De Morgan’s laws:
\[
(a\cup B)^c = A^c\cap B^c \text{ and } (A\cap B)^c = A^c\cup B^c
\]
Arbitrary many unions and intersections: Let $\mathcal{C}$ be a collection of sets. Then
\[
\bigcup\mathcal{C} = \bigcup_{A\in  \mathcal{C}}A = \{x: x \in A\text{, for some } A \in \mathcal{C}\}
\]
\[
\bigcap\mathcal{C} = \bigcap_{A\in  \mathcal{C}}A = \{x: x \in A\text{, for all } A \in \mathcal{C}\}
\]

\subsection{Relations}

Any subset of the Cartesian product of two sets $X$, $Y$ defines a (binary) relation $R \subseteq X \times Y $ between these two sets. Given $(x, y) \in R$ we may denote this inclusion simply as $xRy$. 
   Notation: $\forall$ means ‘for all’, $\exists$ means ‘exists’.
A binary relation $R$ is \textit{univalent} if
\[
\forall x \in X, \forall y \in Y, \forall z \in Y \text{ we have } ((x, y) \in R \text{ and } (x, z)\in R) \implies y = z
\]

A binary relation $R$ is \textit{total} if
\[
\forall x \in X, \exists y \in Y \text{ we have } (x,y) \in R
\]

\begin{itemize}
\item \textbf{Def.:} A partially defined function is a univalent binary relation, and a function is a univalent and total binary relation. Thus a function $f: X \mapsto Y$ is defined by a univalent and total $xRy\iff y = f(x)$.
\end{itemize}
The set of all functions from $X$ to $Y$ is commonly denoted as 
\[
Y^X = \prod_{x\in X}Y
\]

\subsection{Orders and equivalences}
\begin{itemize}
\item \textbf{Def.:} An order $\leq$ on a set $X$ is a binary relation on $X$, s.t. for every $x, y, z \in X$:
\begin{enumerate}
    \item $x \leq x$ (reflexivity),
    \item If $x \leq y$ and $y \leq x$ then $x=y$ (antisymmetry),
    \item If $x \leq y$ and $y\leq z$ then $x\leq z$ (transitivity).
\end{enumerate}
\end{itemize}

An order is \textit{linear} or \textit{total} if $\forall x,y\in X$ either $x\leq y$ or $y\leq x$. If $\leq$ is an order, then we define a strict order by $x < y$ if $x\leq y$ and $x\neq y$.

If for a relation $\sim$ in 2. instead of antisymmetry we have \textit{symmetry}: 
If $x\sim y$ then $y\sim x$
then $\sim$ is called an equivalence relation.

\section{Functions}

Per definition a function $f: X\mapsto Y$ is a univalent and total relation, that is for every $x\in X$ there is a unique $y = f(x)\in Y$. $Do(f) = X$ is called the \textit{domain} of $f$, and $Ran(f) = \{y \in Y: \exists x \in X, y = f(x)\}\subseteq Y$ is called the range of $f$. Also $f(A) = \{y\in Y:\exists x\in A, y = f(x)\}$ for some $A\subseteq X$.

\begin{itemize}
\item \textbf{Ex.:} The identity function $id_X:X$ on a set $X$ is the function that maps every element of $X$ to itself, that is $id_X(x) = x$ for all $x\in X$.

\item \textbf{Ex.:} the characteristic or indicator function $\chi_A:X\mapsto \{0, 1\}$ of $A\subseteq X$ is defined as
\[
\chi_{A}(x) =
\begin{cases}
1, & x \in A,\\
0, & x \notin A.
\end{cases}
\]
The graph of a function $f: X\mapsto Y$ is defined as
\[
G_f= \{(x, y)\in X\times Y:y = f(x)\}
\]
\end{itemize}

\subsection{Properties of functions}

A function $f: X\mapsto Y$ is
\begin{enumerate}
    \item injective (one-to-one) if it maps distinct elements to distinct elements, that is $x_1, x_2\in X$ and $x_1\neq x_2$ implies that $f(x_1)\neq f(x_2)$,
    \item surjective (onto) if its range $Ran(f)$ = Y, that is for every $y$ there exists an $x$, s.t. $y=f(x)$.
    \item If a function is both injective and surjective then its bijective.
\end{enumerate}

We define the composition $f\circ g(z) = f(g(z))$ of functions $f: Y\mapsto X$ and $g: Z\mapsto Y$. Note that we need the inclusion $Ran(g)\subseteq Do(f)$. $\circ$ is associative.

A bijective function $f: X\mapsto Y$ has an inverse $f^{-1}: Y\mapsto X$ defined by
\[
f^{-1}(y) = x \text{ if and only if } f(x) = y
\]
   that is $f\circ f^{-1} = id_Y$ and $f^{-1}\circ f = id_X$.
   
If $f: x\mapsto Y$ is merely injective than still $f: X\mapsto Ran(f)$ is bijective, thus invertible on its range with inverse $f^{-1}: Ran(f)\mapsto X$.

\subsection{Groups, monoids, fields}

\begin{itemize}
\item \textbf{Def.:} Given a function $f: X\times X \mapsto X$ we may denote $f(x, y) = x * y$ and consider this as a binary operation on $X$. For example addition of integers is such an operation. The we say that * is/has

\begin{enumerate}
    \item Associative, if $x * (y * z) = (x * y) * z$,
    \item Commutative, if $x * y = y * x$,
    \item Neutral element, if there exists $e\in X$ (a neutral element), s.t. $x*e=e*x=x$,
    \item Inverse elements, if for all $x\in X$ there exists $x'\in X$ called an inverse of $x$, s.t. $x*x'=x'*x =e$ where $e$ is a neutral element.
\end{enumerate}

\item \textbf{Def.:} $(X, *)$ is called a
\begin{enumerate}
    \item Semigroup, if * is associative,
    \item Monoid, if $(X, *)$ is a semigroup and has a neutral element,
    \item Group, if $(X, *)$ is a monoid and every element $x\in X$ has an inverse.
\end{enumerate}

\item \textbf{Theorem:} In a group $(X, *)$ the neutral element $e\in X$ and inverse $x'$ for any fixed $x\in X$ are unique.

\item \textbf{Proof.:} Indeed, if there would be two neutral elements $e, e'$, then $e'=e*e'=e$. Also assuming $x*y=e=x*z$, implies $x'*(x*y)=x'*(x*z)$, that is $y=z=x'$.

Let $(X, \cdot, +)$ be given with binary operations $\cdot$ and $+$.

\item \textbf{Def.:} $(X, \cdot, +)$ is a field if
\begin{enumerate}
    \item $(X, \cdot, +)$ is a commutative group with neutral element 0,
    \item $(X\setminus \{0\}, \cdot)$ is a commutative group,
    \item $\cdot$ distributes over $+$ (distributivity), that is: $x\cdot (y+z) = x\cdot y + x\cdot z$.
In this case $+$ is usually called addition and $\cdot$ multiplication.
\end{enumerate}

\item \textbf{Ex.:} the rational numbers $\mathbb{Q}$ is a field, moreover an ordered field $(\mathbb{Q}, \cdot, +, \leq)$ equipped with the total order $x\leq y\iff0\leq y-x$, where 0 is the neutral element of $+$.

\item \textbf{Ex.:} the set of real numbers $\mathbb{R}$ is also a totally ordered field $(\mathbb{R}, \cdot, +, \leq)$

\item \textbf{Axiom:} the $\leq$ order of $\mathbb{R}$ satisfies
\begin{enumerate}[label=\Roman*.]
  \item $x\leq y$ implies $x+z\leq y+z$,
  \item $x< y$ and $z>0$ implies $xz<yz$.
\end{enumerate}
\end{itemize}
\subsection{Supremum and infimum}

\begin{itemize}
\item \textbf{Def.:} A set $A\subseteq \mathbb{R}$ is \textit{bounded} \textit{from above}, if $\exists M\in \mathbb{R}$ s.t.  $x\leq M$ for all $x\in A$; and it is \textit{bounded from below}, if $\exists m\in \mathbb{R}$ s.t.  $x\geq m$ for all $x\in A$. If both holds for $A$, then it is bounded.				(‘s.t.’ is short hand for ‘such that’)

\item \textbf{Def.:} If $M\in \mathbb{R}$ is an upper bound of $A\subseteq \mathbb{R}$ s.t. for any other upper bound $M'\in \mathbb{R}$ of $A$ we have $M\leq M'$, then $M$ is called the least upper bound of $A$, denoted as
\[
M=\sup A
\]
Similarly, the greatest lower bound of $A\subseteq \mathbb{R}$, if exists, is denoted by 
\[
m=\inf A
\]
  meaning $m\geq m'$ for any lower bound $m$ of $A$.
If $A=\{x_i: i\in I\}\subseteq \mathbb{R}$ for an index set $I$, we also write:
\[
\sup A=\sup_{i\in I}x_i \text{ and } \inf A = \inf_{i\in I}x_i
\]

\textbf{Fact:} by the definition supremum and infimum of a set, if they exist, are both unique and $\sup A\geq \inf A$ for nonempty $A\subseteq \mathbb{R}$.

\item \textbf{Def.:} if $\sup A\in A$, then we call it the maximum of $A$ denoted by $\max A$, similarly if $\inf A \in A$, then we call it the minimum of $A$ denoted by $\min A$.

\textbf{Ex.:} Let $\mathbb{R}\supseteq A=\{{1\over n} : n\in \mathbb{N}\}$. Then $\sup A=1$ belongs to $A$, while $\inf A=0$ does not belong to $A$.

\item \textbf{Def.:} let us introduce the elements $\infty$,$-\infty$, so that $\infty >x>-\infty$ for any $x\in \mathbb{R}$ and define the extended real numbers as $\overline{\mathbb{R}}=\{\infty, -\infty\}\cup \mathbb{R}$. If a set $A\subseteq \mathbb{R}$ is not bounded from above then define $\sup A=\infty$, and if $A\subseteq \mathbb{R}$ is not bounded from below then define $\inf A=-\infty$. Also define $\sup \emptyset=-\infty$ and $\inf \emptyset = \infty$.
\end{itemize}

\subsection{(Order) Completeness}

Consider $A=\{x\in \mathbb{Q}:x^2\leq 2\}$. This set is bounded from above but has no least upper bound in $\mathbb{Q}$.

\begin{itemize}
\item \textbf{Def(Completeness).:} a totally ordered field $Z$ is complete, if all nonempty upper bounded subsets of $Z$ have a least upper bound in $Z$. We call this the least upper bound property.

\item \textbf{Theorem(Dedekind):} There exists a unique (up to $(\cdot, +,\leq)$-preserving transformation) ordered complete field satisfying the order axioms I., II. that contains $\mathbb{Q}$ and it is the field $\mathbb{R}$.
Such a transformation $\phi:\mathbb{R}\mapsto \mathcal{M}$ satisfies $\phi(x+y)=\phi(x)+\phi(y), \phi(xy)=\phi(x)\phi(y), x\leq y \implies \phi(x)\leq \phi(y)$.
\end{itemize}

\subsection{Archimedean property}

\begin{itemize}
\item \textbf{Theorem(Archimedean property):} If $x\in \mathbb{R}$, then there exists $n\in \mathbb{Z}$ such that $x<n$.

\item \textbf{Proof:} Suppose, for contradiction, that there exists $x\in \mathbb{R}$ s.t. $x>n$ for all $n\in \mathbb{Z}$. Then $x$ is an upper bound of $\mathbb{Z\subseteq\mathbb{R}}$, so $M=\sup\mathbb{Z}\in\mathbb{R}$ exists. Since $n\leq M$ for all $n\in \mathbb{Z}$, we have $n-1\leq M-1$ for all $n\in \mathbb{Z}$, which implies $n\leq M-1$ for all $n\in \mathbb{Z}$. But then $M-1$ is an upper bound of $\mathbb{Z}$ that is strictly less than $M=\sup \mathbb{Z}$, a contradiction to $M=\sup \mathbb{Z}$ being the least upper bound.

\item \textbf{Corollary:} For every $0<\varepsilon\in \mathbb{R}$, there exists an $n\in \mathbb{N}$, s.t. $0<{1\over n}<\varepsilon$.

\item \textbf{Corollary(integer part):} If $x\in \mathbb{R}$, then there exists $[x]=n\in \mathbb{Z}$ called the integer part of $x$, such that $n\leq x<n+1$.
\end{itemize}

\subsection{Further properties}

\begin{itemize}
\item \textbf{Def(dense set):} $A\subseteq \mathbb{R}$ is dense in $\mathbb{R}$, if for any $0<\varepsilon, x\in \mathbb{R}$ there exists $a\in A$, s.t. $x-\varepsilon<a<x+\varepsilon$.

\item \textbf{Theorem(density of rationals):} $\mathbb{Q\subseteq\mathbb{R}}$ is dense in $\mathbb{R}$.

\item \textbf{Proof:} Let $0<\varepsilon, x\in \mathbb{R}$. Then for any $n\in \mathbb{N}$ we have 
\[
[nx]\leq nx<[nx] + 1
\]
which gives
\[
{[nx]\over n}\leq x <{[nx]\over n} + {1\over n}
\]
Pick $n\in \mathbb{N}$, s.t. $0<{1\over n}<\varepsilon$. Then we have
\[
{[nx]\over n}\leq x <{[nx]\over n} +{1\over n}<{[nx]\over n} +\varepsilon
\]
which implies $x-\varepsilon<{[nx]\over n} <x+\varepsilon$ as wanted.
\end{itemize}

\subsection{Properties of sup and inf}

\textbf{Theorem:} 
\begin{enumerate}
    \item Given $A\in \mathbb{R}$, then $M=\sup A$ if and only if
    \begin{enumerate}
        \item $M$ is an upper bound of $A$, 
        \item for every $M' < M$ there exists $x\in A$ s.t. $M'<x$.
    \end{enumerate}
    \item If $A\subseteq B\subseteq \mathbb{R}$, then $\sup A\leq \sup B$ and $\inf B\leq \inf A$.
    \item If $A\subseteq \mathbb{R}$, then $\inf A\leq \sup A$.
    \item If $A\subseteq \mathbb{R}$, then $-\inf A = \sup (-A)$.
    \item If $A\subseteq \mathbb{R} \ni \alpha\geq 0$, then $\sup (\alpha A)=\alpha\sup (A)$.
    \item If $A, B \subseteq \mathbb{R}$, we have $\sup (A+B)\leq \sup A + \sup B$, $\inf (A+B)\geq \inf A+\inf B$ where $A+B=\{x\in \mathbb{R}:x=a+b, a\in A, b\in B\}$.
    \item Let $\mathcal{C}$ be a family of sets in $\mathbb{R}$, then $\sup (\cup \mathcal{C}) = \sup \{\sup A:A\in \mathcal{C}\}$.
\end{enumerate}

\subsection{\texorpdfstring{Intervals and topology of $\mathbb{R}$}{Intervals and topology of R}}

\textbf{Def.:} Let $a, b \in \mathbb{R}$.
\begin{enumerate}
    \item Closed interval $[a, b]=\{x\in \mathbb{R}:a\leq x\leq b\}$,
    \item Open interval $(a, b)=\{x\in \mathbb{R}:a<x<b\}$,
    \item Half-open intervals $(a, b\text{]}=\{x\in \mathbb{R}:a<x\leq b\}, \text{[}a, b)=\{x\in \mathbb{R}:a\leq x < b\}$
    \item $\text{[}a, \infty)=\{x\in \mathbb{R}:a\leq x\}, (a, \infty)=\{x\in \mathbb{R}:a<x\}, (-\infty, b\text{]}=\{x\inf \mathbb{R}:b\geq x\}, (-\infty, b)=\{x\in \mathbb{R}:b>x\}$
\end{enumerate}

\textbf{Def.:} $A\subseteq \mathbb{R}$ is open if for every $x\in A$ there exists $0<\epsilon \in \mathbb{R}$ s.t. $(x-\epsilon, x+\epsilon)\subseteq A$.

\textbf{Def.:} $B\subseteq \mathbb{R}$ is closed if $B^C=\{x\in \mathbb{R}:x\notin B\}$ is open.

\textbf{Def.:} $U\subseteq \mathbb{R}$ is a neighborhood of $z\in \mathbb{R}$, if there exists $0<\epsilon \in \mathbb{R}$ s.t. $(z-\epsilon, z+\epsilon)\subseteq U$.

\textbf{Theorem:} Arbitrary union of open sets is open, and an intersection of finite number of open sets is open.

\textbf{Def.:} Let $A\subseteq \mathbb{R}$, then $x\in \mathbb{R}$ is
\begin{enumerate}
    \item an interior point of $a$, if there exists $0<\epsilon \in \mathbb{R}$ s.t. $(x-\epsilon, x+\epsilon)\subseteq A$;
    \item an isolated point of $A$, if $x\in A$ and there exists $0<\epsilon \in \mathbb{R}$ s.t. $x$ is the only point of $A$ that belongs to $(x-\epsilon, x+\epsilon)$; 
    \item a boundary point of $A$, if for every $0<\epsilon \in \mathbb{R}$ the interval $(x-\epsilon, x+\epsilon)$ contains at least a point in $A$ and at least a point not in $A$;
    \item an accumulation point of $A$, if for every $0<\epsilon \in \mathbb{R}$ the interval $(x-\epsilon, x+\epsilon)$ contains a point in $A$ distinct from $x$. 
\end{enumerate}

\section{The absolute value}
\begin{itemize}
\item \textbf{Def.:} the absolute value of $x \in \mathbb{R}$ is defined by 
$\begin{cases}
x \text{ if } x \geq 0,\\
-x \text{ if } x < 0.
\end{cases}.$
\item \textbf{Proposition}: $\forall x, y \in \mathbb{R}$ we have 
\begin{enumerate}
    \item $|x| \geq 0$ and $|x| = 0 \iff x = 0$,
    \item $|-x| = |x|$,
    \item (triangle inequality) $|x + y| \leq |x| + |y|$,
    \item $|xy| = |x||y|$,
    \item $||x| - |y|| \leq |x - y|$.
\end{enumerate}

\item \textbf{Proof}: 1.,2. and 4. are trivial. To see 3. suppose without loss of generality that $x \geq 0$, $|x| \geq |y|$, in which case $x + y \geq 0$. If $y \geq 0$, then $|x + y| = x + y = |x| + |y|$. If $y < 0$, then $|x + y| = x + y = |x| - |y| \leq |x| + |y|$. To obtain 5. we use 3. to get $|x| = |x - y + y| \leq |x - y| + |y|$.
\end{itemize}

\section{Sequences and limits}

\begin{itemize}
\item \textbf{Def.:} a sequence $x_n$ of real numbers is an ordered list of numbers $x_n \in \mathbb{R}$, called the terms of the sequence, indexed by the natural numbers $n \in \mathbb{N}$. It may be regarded as a function $f: \mathbb{N} \to \mathbb{R}$ with $x_n = f(n)$.

\item \textbf{Def.:} A sequence $(x_n)$ of real numbers converges to a limit $x \in \mathbb{R}$, written as $\displaystyle x = \lim_{n\to\infty} x_n$, or $x_n \to x$ as $n\to\infty$, if $\forall \varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $\forall n > N$ we have $|x_n - x| < \varepsilon$.

A sequence $(x_n)$ converges if it converges to a limit $x \in \mathbb{R}$, otherwise it diverges. Note that $x_n \to x$ and $|x_n - x| \to 0$ are equivalent statements.

\item \textbf{Def.:} if $(x_n)$ is a sequence, then $\displaystyle \lim_{n\to\infty} x_n = \infty$, or $x_n\to\infty$ if $\forall M \in \mathbb{R}$ there exists $N \in \mathbb{N}$ s.t. $\forall n > N$ we have $x_n > M$. Similarly, we define $\displaystyle \lim_{n\to\infty} x_n = -\infty$, or $x_n \to -\infty$ if $\forall M \in \mathbb{R}$ there exists $N \in \mathbb{N}$ s.t. $\forall n > N$ we have $x_n < M$.
\end{itemize}

\subsection{Properties of limits}

\begin{itemize}
\item \textbf{Proposition}: If a sequence converges, then its limit is unique. 

\item \textbf{Proof}: Suppose that $(x_n)$ is a sequence such that $x_n \to x$ and $x_n \to x'$ as $n\to\infty$. Then $\forall \varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $|x_n - x| < \varepsilon/2$ and $|x_n - x'| < \varepsilon/2$ for all $n > N$. Then $|x - x'| = |x - x_n + x_n - x'| \leq |x - x_n| + |x_n - x'| < \varepsilon$ for all $n > N$. Since $\varepsilon > 0$ was arbitrary, $|x - x'| < \varepsilon$ proves that $|x - x'| = 0$.

\item \textbf{Ex}.: Let $x_n = \frac{1}{n} : n \in \mathbb{N}$. Then $x_n \to 0$. Indeed, let $\varepsilon > 0$ be given. Choose $N \in \mathbb{N}$ s.t. $N > \frac{1}{\varepsilon}$. Then $\forall n > N$ we have $|\frac{1}{n} - 0| = \frac{1}{n} < \frac{1}{N} < \varepsilon$, which proves that $\displaystyle \lim_{n\to\infty} \frac{1}{n} = 0$.

\item \textbf{Proposition}: A convergent sequence is bounded.

\item \textbf{Proof}: After some index $N \in \mathbb{N}$ for all $n > N$ we have $|x_n - x| < 1$ which implies that $x - 1 < x_n < x + 1$, thus $x_n$ is bounded.

\item \textbf{Fact}: Convergence of $x_n$ to $x$ does not depend on any of the first finitely many elements of $(x_n)$.

\item \textbf{Theorem}: If $(x_n)$, $(y_n)$ are convergent sequences, then $x_n \leq y_n$ for all $n > N \in \mathbb{N}$ implies $\lim_{n\to\infty} x_n \leq \lim_{n\to\infty} y_n$. 

\item \textbf{Proof}: $\forall \varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $|x_n - x| < \varepsilon/2$ and $|y_n - y| < \varepsilon/2$ for all $n > N$. Then $x = x_n + x - x_n < y_n + \frac{\varepsilon}{2} = y + y_n - y + \frac{\varepsilon}{2} < y + \varepsilon$, which implies $x \leq y$.

\item \textbf{Theorem(Squeeze, or Sandwich)}: If $(x_n)$, $(y_n)$ are convergent sequences with common limit $L$, then $x_n \leq z_n \leq y_n$ implies $\displaystyle \lim_{n\to\infty} z_n = L$ as well. 

\item \textbf{Proof}: The assumption implies that $\forall \varepsilon > 0$ there exists an index $N \in \mathbb{N}$ s.t. for all $n > N$ we have $L - \varepsilon < x_n \leq z_n \leq y_n < L + \varepsilon$, which means that $z_n \to L$. 

\item \textbf{Theorem}: If $(x_n)$, $(y_n)$ are convergent sequences and $c \in \mathbb{R}$, then
\begin{enumerate}
    \item $\displaystyle \lim_{n\to\infty} cx_n = c \lim_{n\to\infty} x_n$; 
    \item $\displaystyle \lim_{n\to\infty} (x_n + y_n) = \lim_{n\to\infty} x_n + \lim_{n\to\infty} y_n$; 
    \item$\displaystyle \lim_{n\to\infty} (x_n y_n) = \lim_{n\to\infty} x_n \lim_{n\to\infty} y_n$.
\end{enumerate}
\item \textbf{Proof}: If c = 0 then 1. is immediate. If $c \neq 0$, then $|x_n - x| < \varepsilon/|c|$ for all $n > N$ implies $|cx_n - cx| < \varepsilon$.
\end{itemize}

For the second statement we have $|(x_n + y_n) - (x + y)| \leq |x_n - x| + |y_n - y| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon$ for all large enough n, which proves that $x_n + y_n \to x + y$.

For the third statement $|x_n y_n - xy| = |(x_n - x)y_n + x(y_n - y)| \leq |x_n - x||y_n| + |y_n - y||x| < \varepsilon$ for all large enough n, which proves that $x_n y_n \to xy$.

\subsection{Monotone sequences}
    \begin{itemize}
        \item \textbf{Definition:} A sequence $(x_n)$ is monotone
              \begin{description}[leftmargin=1.8em, labelsep=0.5em, style=nextline]
                  \item[Increasing:] if $x_n \le x_{n+1}$, \emph{strictly} if $x_n < x_{n+1}$;
                  \item[Decreasing:] if $x_n \ge x_{n+1}$, \emph{strictly} if $x_n > x_{n+1}$.
              \end{description}
        \item \textbf{Monotone Convergence Theorem:}
        
        If $(x_n)$ is monotone increasing, then $\displaystyle \lim_{n\to\infty} x_n = \sup\{x_n : n \in \mathbb N\}$.

        If $(x_n)$ is monotone decreasing, then $\displaystyle \lim_{n\to\infty} x_n = \inf\{x_n : n \in \mathbb N\}$.
        \item \textbf{Proof:} We prove only the first statement, the second can be proved by choosing $y_n = -x_n$. The least upper bound of the set $\{x_n:n \in \mathbb N\}$ is $M = \sup\{x_n:n \in \mathbb N\}$, so $x_n \le M$. Suppose $M < \infty$. Then $\forall \varepsilon > 0$ there exists $N \in \mathbb N$ s.t. $M - \varepsilon < x_N \le x_n$ where the second inequality holds for all $n > N$ by monotonicity. This implies $M - \varepsilon < x_n < M + \varepsilon$ for all $n > N$, thus $\lim_{n\to\infty} x_n = M$. If $M = \infty$, then still $\forall K > 0$ there exists $N \in \mathbb N$ s.t. $K < x_N \le x_n$ for all $n > N$ by monotonicity. Thus, $\lim_{n\to\infty} x_n = \infty$.
    \end{itemize}

\subsection{\texorpdfstring{$\lim sup x_n$ and $\lim inf x_{n}$}{lim sup xn and lim inf xn}}
\begin{itemize}
    \item \textbf{Def.:} for a sequence $(x_n)$ we define
        \begin{description}
            \item $\displaystyle \lim_{n\to\infty}sup x_n = \lim_{n\to\infty} y_n$ where $y_n = \sup\{x_k: k \in \mathbb{N}, k \ge n\}$; 
            \item $\displaystyle \lim_{n\to\infty}inf x_n = \lim_{n\to\infty} z_n$ where $z_n = \inf\{x_k: k \in \mathbb{N}, k \ge n\}$.
        \end{description}
    \item Note that the above limits exist, because $y_n \ge y_{n+1}$ and $z_n \le z_{n+1}$.
    \item \textbf{Theorem:} We have $\displaystyle y = \lim_{n\to\infty} sup x_n \iff -\infty \leq y \leq \infty$ satisfies one of the following: 
    \begin{enumerate}
        \item $-\infty < y < \infty$ and for $\forall \varepsilon > 0$
        \begin{enumerate}[label=(\alph*)]
            \item there exists $N \in \mathbb{N}$ s.t. for all $n > N$ we have $x_n < y + \varepsilon$; 
            \item For every $N \in \mathbb{N}$ there exists $n > N$ s.t. $x_n > y - \varepsilon$. 
        \end{enumerate}
        \item $y = \infty$ and for every $M \in \mathbb{R}$ there exists $n \in \mathbb{N}$ s.t. $x_n > M$. 
        \item $y = -\infty$ and for every $m \in \mathbb{R}$ there exists $N \in \mathbb{N}$ s.t. $x_n < m$ for all $n > N$. 
    \end{enumerate}
    Analogous results hold for the lim inf as well.
    \item \textbf{Proof:} First suppose that $-\infty < y < \infty$. Then $(x_n)$ is bounded from above and $y_n = \sup\{x_k: k \in \mathbb{N}, k \ge n\}$ is a monotone decreasing sequence with limit $y$. Therefore $\forall \varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $y_N < y + \varepsilon$. Since $x_n \le y_N$ for all $n > N$ we have $x_n < y + \varepsilon$ proving 1.a. 
    
    To prove 1.b. let $\varepsilon > 0$ and suppose that $N \in \mathbb{N}$ is arbitrary. $y_N \ge y$ is the sup of $\{x_k: k \in \mathbb{N}, k \ge N\}$, there exists $n \ge N$ s.t. $x_n > y_N - \varepsilon \ge y - \varepsilon$ which proves 1.b. 
    
    Conversely, suppose that $-\infty < y < \infty$ satisfies 1. Then given any $\varepsilon > 0$, 1.a. implies that there exists $N \in \mathbb{N}$ s.t. $y_n = \sup\{x_k: k \in \mathbb{N}, k \ge n\} < y + \varepsilon$ for all $n > N$, and 1.b. implies that $y_n > y - \varepsilon$ for all $n \in \mathbb{N}$. Thus, $|y_n - y| < \varepsilon$ for all $n > N$, so $y_n \to y$.

    \item \textbf{Theorem:} for a sequence $(x_n)$ we have 
    
    $\displaystyle \limsup_{n\to\infty} x_n = \liminf_{n\to\infty} x_n = x \iff \lim_{n\to\infty} x_n = x.$. 
    \item \textbf{Proof:} First suppose $\displaystyle \limsup_{n\to\infty} x_n = \liminf_{n\to\infty} x_n = x$. Then $y_n = \sup\{x_k: k \in \mathbb{N}, k \geq n\}$ is monotone decreasing to $x$, while $z_n = inf{x_k: k \in \mathbb{N}, k \geq n}$ is monotone increasing to $x$ and also $z_n \leq x_n \leq y_n$, so the squeeze theorem proves $\displaystyle \lim_{n\to\infty} x_n = x$. The reverse implication follows from $x - \varepsilon < x_n < x + \varepsilon$ implying also $x - \varepsilon < z_n \leq y_n < x + \varepsilon$ for all $n > N$ where $N$ is chosen accordingly to given $\varepsilon > 0$. Thus, $z_n, y_n \to x$.

    In the remaining cases a sequence $(x_n)$ diverges to $\displaystyle \infty \iff \liminf_{n\to\infty} x_n = \infty$, and then $\displaystyle \limsup_{n\to\infty} x_n = \infty$, since $\displaystyle \limsup_{n\to\infty} x_n \geq \liminf_{n\to\infty} x_n$. Similarly, $(x_n)$ diverges to $\displaystyle -\infty \iff \limsup_{n\to\infty} x_n = -\infty$ and then $\displaystyle \liminf_{n\to\infty} x_n = -\infty$ as well. 
    \item \textbf{Corollary:} $\displaystyle \lim_{n\to\infty} x_n = x \iff \limsup_{n\to\infty} |x_n - x| = 0$.
\end{itemize}

\subsection{Cauchy sequences}
\begin{itemize}
    \item \textbf{Def(Cauchy sequence):} a sequence $(x_n)$ is a Cauchy sequence if for every $\varepsilon > 0$ there exists $N \in \mathbb{N}$ such that $|x_n - x_m| < \varepsilon$ for all $n, m > N$. 
    \item \textbf{Theorem:} A sequence $(x_n)$ converges $\iff$ it is Cauchy.
    \item \textbf{Proof:} First suppose that $(x_n)$ converges to a limit $x \in \mathbb{R}$. Then $\forall \varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $|x_n - x| < \frac{\varepsilon}{2}$ for all $n > N$. It follows that if $n, m > N$, then $|x_n - x_m| \leq |x_n - x| + |x_m - x| < \varepsilon$ proving that $(x_n)$ is Cauchy.

    Conversely suppose that $(x_n)$ is Cauchy. Then there exists $N_1 \in \mathbb{N}$ s.t. for all $n, m > N_1$, we have that $|x_n - x_m| < 1$. Then for $n > N_1$ we have $|x_n| \leq |x_n - x_{N_1 + 1}| + |x_{N_1 + 1}| \leq 1 + |x_{N_1 + 1}|$. 
    
    Thus, the sequence is bounded with 
    
    $|x_n| \leq \max\{|x_1|, ... , |x_{N_1}|, 1+|x_{N_1 + 1}|\}$.
    
    Given that $(x_n)$ is bounded, its lim sup and lim inf is bounded. Given $\varepsilon > 0$, choose $N \in \mathbb{N}$ s.t. by the assumption $x_n - \varepsilon < x_m < x_n + \varepsilon$ for all $m \geq n > N$. Then we have 
    
    $x_n - \varepsilon \leq \inf\{x_m: m \in \mathbb{N}, m \geq n\}$ and $\sup\{x_m: m \in \mathbb{N}, m \geq n\} \leq x_n + \varepsilon$,
    
    which implies 
    
    $\sup\{x_m: m \in \mathbb{N}, m \geq n\} - \varepsilon \leq \inf\{x_m: m \in \mathbb{N}, m \geq n\} + \varepsilon$. 
    
    Taking the limit $n \to \infty$, we get that 
    
    $\displaystyle \limsup_{n \to \infty} x_n - \varepsilon \leq \liminf_{n \to \infty} x_n + \varepsilon$ 
    
    and since $\varepsilon > 0$ was arbitrary, this yields $\displaystyle \limsup_{n \to \infty} x_n \leq \liminf_{n \to \infty} x_n$, thus 
    
    $\displaystyle \limsup_{n \to \infty} x_n = \liminf_{n \to \infty} x_n$ which implies convergence.
\end{itemize}

\subsection{Subsequences}
\begin{itemize}
    \item \textbf{Def(subsequence):} a subsequence of a sequence $(x_n)$ is a sequence of the form $(x_{n_k})$ with index $k \in \mathbb{N}$ where $n_1 < n_2 < ... < n_k < ....$ In other words, $n_k$ is a strictly increasing integer valued function with domain $\mathbb{N}$.
    \item \textbf{Proposition:} Every subsequence of a convergent sequence converges to the limit of the sequence.
    \item \textbf{Proof:} Suppose $(x_n)$ is a convergent sequence with $\displaystyle \lim_{n\to\infty} x_n = x$ and $(x_{n_k})$ is a subsequence. Given $\varepsilon > 0$ there exists $N \in \mathbb{N}$ s.t. $|x_n - x| < \varepsilon$ for all $n > N$. Since $n_k \to \infty$ as $k \to \infty$, there exists $K \in \mathbb{N}$ s.t. $n_k > N$ if $k > K$. Then $k > K$ implies $|x_{n_k} - x| < \varepsilon$.
    \item \textbf{Corollary:} If a sequence has subsequences that converge to different limits, then the sequence diverges.
\end{itemize}

\subsection{Bolzano-Weierstrass theorem}
\begin{itemize}
    \item \textbf{Theorem(Bolzano-Weierstrass):} Every bounded sequence of real numbers has a convergent subsequence. 
    \item \textbf{Proof:} The statement will follow from the Monotone Convergence theorem of monotone bounded sequences using the following 
    
    \textbf{Claim:} every sequence $(x_n)$ has a monotone subsequence. 

    \textbf{Proof:} Let us call a positive integer-valued index $n$ of a sequence a "peak" of the sequence when $x_m \leq x_n$ for all $m > n$. Suppose first that the sequence has infinitely many peaks, which means there is a subsequence $(x_{n_k})$ consisting of these peaks for which we have $x_{n_1} \geq x_{n_2} \geq x_{n_3} \geq ... \geq x_{n_k} \geq ...$, so $(x_{n_k})$ is a monotone decreasing subsequence. Now suppose that on the contrary there are only finitely many peaks of $(x_n)$. Let the index $N$ be the index of the final peak in $(x_n)$ and let $n_1 = N + 1$. Since $n_1$ comes after the final peak, it is not a peak, thus it implies the existence of $n_2 > n_1$ s.t. $x_{n_2} \geq x_{n_1}$. Again $n_2$ comes after the final peak, it is not a peak, so again it implies the existence $n_3 > n_2$ s.t. $x_{n_3} \geq x_{n_2}$. Repeating this we obtain $x_{n_1} \leq x_{n_2} \leq x_{n_3} \leq ... \leq x_{n_k} \leq ...$ a monotone increasing subsequence $(x_{n_k})$.
\end{itemize}

\section{Binomial theorem}
\begin{itemize}
    \item Let $\dbinom{n}{k} = B(n,k) = \frac{n!}{k!(n-k)!}$ for $n \geq k \geq 0$ integers. It is called the binomial coefficient.
    \item \textbf{Proposition:} $\dbinom{n}{k}$ gives the number of ways, disregarding order, that k objects can be chosen from among $n$ objects; more formally, the number of $k$-element subsets (or $k$-combinations) of an $n$-element set. Also, $\dbinom{n}{k} = \dbinom{n-1}{k-1} + \dbinom{n-1}{k}$.
    \item \textbf{Theorem(Binomial theorem):} Let $x, y \in \mathbb{R}$ and $n \in \mathbb{N}_0$. Then 
    \[
    (x+y)^n = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k}.
    \]
    \item \textbf{Proof:}~$(x+y)^n = \underbrace{(x+y)}_{1}\,\cdots\,\underbrace{(x+y)}_{n}.$

    and you count the number of times the term $x^k y^{n-k}$ occurs in the expansion. It is exactly how many ways $k$ objects can be chosen from among $n$ objects.
    \item \textbf{Corollary:} For $1 < x = 1 + \delta$ the binomial theorem implies that $x^n > 1 + n\delta$, thus $x^n \to \infty$. Similarly, when $1 > x > 0$ we have that $\frac{1}{x} > 1$, so $\frac{1}{x^n} \to \infty$ implies that $x^n \to 0$. When $-1 < x < 1$ these imply $x^n \to 0$.
\end{itemize}


\section{Series}
\subsection{Series}

\textbf{Def(Convergence of a series).:} Let $(a_n)$ be a sequence of real numbers. The series
\[
\sum^{\infty}_{n=1}{a_n}
\]
converges to a sum $s\in \mathbb{R}$ if the sequence $(s_n)$ of partial sums
\[
s_k=\sum_{n=1}^{k}{a_n}
\]
converges to $s$ as $k\mapsto \infty$. Otherwise, the series diverges. If a series 
converges to $s$, we write 
\[
s=\sum_{n=1}^{\infty}{a_n}
\]
We also say a series diverges to $\pm \infty$ if its sequence of partial sums does.

\subsection{Basic conditions for convergence}
\textbf{Proposition:} a series $\sum_{n=1}^{\infty}{a_n}$  with $a_n\geq 0$ converges iff $\exists M\geq 0$ s.t.
\[
s_k=\sum_{n=1}^{k}{a_n}\leq M
\]
for all $k\in \mathbb{N}$, that is $\{s_k\}_{k\in \mathbb{N}}$ is bounded from above; otherwise it 
diverges to $\infty$.

\textbf{Proof:} $a_n\geq 0$ implies that $(s_k)$ is monotone increasing, so by the Monotone Convergence Theorem its limit exists and is equal to $\sup \{s_k:l\in \mathbb{N}\}$.

\textbf{Theorem(Cauchy condition):} $\sum_{n=1}^{\infty}{a_n}$ converges iff $\forall \epsilon>0$ there exist $N\in \mathbb{N}$ s.t. $|\sum_{n=m+1}^{k}{a_n}| < \epsilon$ for all $m,k>N$.

\textbf{Proof:} $(s_k)$ converges iff $(s_k)$ is Cauchy, that is: $\forall \epsilon > 0$ there exist $N\in \mathbb{N}$ s.t. $|s_k-s_m|<\epsilon$ for all $m,k>N$ where $|s_k-s_m|=|\sum_{n=m+1}^{k}{a_n}|$.


\textbf{Proposition:} if $\sum_{n=1}^{\infty}{a_n}$ converges, then $\lim_{n\to \infty}a_n=0$.

\textbf{Proof:} the series converges iff $(s_k)$ is Cauchy, so taking $m=k-1$ in the Cauchy condition we obtain that $|a_k|<\epsilon$ for all $k>N$, thus $a_k\to 0$.

\textbf{Note:} the converse of the above is not true! We will see soon that $a_n/to 0$ generally does not imply that $\sum_{n=1}^{\infty}{a_n}$ converges.

\subsection{Absolutely convergent series}

\textbf{Def(Absolute Convergence of a series).:} The series $\sum_{n=1}^{\infty}{a_n}$  converges absolutely if $\sum_{n=1}^{\infty}{|a_n|}$ converges, and $\sum_{n=1}^{\infty}{a_n}$ converges conditionally if $\sum_{n=1}^{\infty}{a_n}$ converges, but $\sum_{n=1}^{\infty}{|a_n|}$ diverges.

\textbf{Proposition:} if $\sum_{n=1}^{\infty}{a_n}$ converges absolutely, then $\sum_{n=1}^{\infty}{a_n}$ converges.

\textbf{Proof:} $\sum_{n=1}^{\infty}{|a_n|}$ is convergent, so $s_k=\sum_{n=1}^{k}{|a_n|}$ is Cauchy, which implies that $|\sum_{n=1}^{k}{a_n}|$ is also Cauchy by $|\sum_{n=m+1}^{k}{a_n}| \leq \sum_{n=m+1}^{k}{|a_n|}$, so the sequence of partial sums of $\sum_{n=1}^{\infty}{a_n}$ is Cauchy.

\subsection{Alternating series}

\textbf{Def(Alternating series).:} A series $\sum_{n=1}^{\infty}{a_n}$  is alternating if $a_na_{n+1} < 0$, that is, successive terms of $(a_n)$ have opposite signs.

\textbf{Theorem(Alternating series test):} Suppose that $(a_n)$ is a monotone decreasing sequence with $a_n\geq 0$ and also $\lim{n\to \infty}a_n=0$. Then the alternating sequence $\sum_{n=1}^{\infty}{(-1)^{n}a_n}$ converges.

\textbf{Proof:} $|\sum_{n=m+1}^{k}{(-1)^{n}a_n}|\leq a_{m+1}$ because $a_n\geq a_{n+1}\geq 0$, so $\lim{m\to \infty}a_{m+1}=0$ implies that the partial sums form a Cauchy sequence.

\textbf{Ex.:} $\sum_{n=1}^{\infty}{(-1)^{n}{1\over n}}$ converges.

\subsection{Rearrangement of a series}

\textbf{Def(rearrangement).:} A series $\sum_{n=1}^{\infty}{b_n}$ is a rearrangement of $\sum_{n=1}^{\infty}{a_n}$ if there is a bijective function $f:\mathbb{N}\mapsto \mathbb{N}$ s.t. $b_n=a_{f(n)}$.

\textbf{Theorem:} if a series is absolutely convergent, then every rearrangement of the series converges to the same sum.

\textbf{Proof:} First suppose that $a_n\geq 0$ and let $\sum_{n=1}^{\infty}{b_n}$  with $b_n=a_{f(n)}$ be a rearrangement. Given $\epsilon > 0$ there exist $n\in \mathbb{N}$ s.t. $0\leq \sum_{n=1}^{\infty}{a_n} - \sum_{n=1}^{N}{a_n}<\epsilon$. Since $f:\mathbb{N} \mapsto \mathbb{N}$ is bijective, there exists $M\in \mathbb{N}$ s.t. $\{1,2,...,N\}\subseteq f^{-1}(\{1,2,...,M\})$ meaning that $a_1, a_2, ..., a_N$ are all included among the $b_1, b_2, ..., b_M$. If $m>M$, then $\sum_{n=1}^{N}{a_n}\leq \sum_{n=1}^{m}{b_n}\leq \sum_{n=1}^{\infty}{a_n}$ and it follows that $0\leq \sum_{n=1}^{\infty}{a_n}-\sum_{n=1}^{m}{b_n}<\epsilon $, which proves the assertion.

The general case of the assertion follows from separating $\sum_{n=1}^{\infty}{a_n}$ into two sums of its positive and negative terms which are separately convergent because of absolute convergence of $\sum_{n=1}^{\infty}{a_n}$. Then any rearrangement splits into the rearrangements of the sums of the negative and positive terms of $\sum_{n=1}^{\infty}{a_n}$, so separately converge to the sums of the negative and positive terms of $\sum_{n=1}^{\infty}{a_n}$.

\textbf{Theorem:} if a series $\sum_{n=1}^{\infty}{a_n}$ is conditionally convergent, then it has rearrangements that converge to an arbitrary real number and rearrangements that diverge to $\infty$ or $-\infty$.

\textbf{Proof:} Since $\sum_{n=1}^{\infty}{a_n}$ is convergent, therefore $a_n\to 0$. Since $\sum_{n=1}^{\infty}{a_n}$ is only conditionally convergent, therefore the sums of its negative and positive parts are both divergent, otherwise the whole series would diverge if only one of them converged. This means that we can make sums of successive positive or negative terms in the series as large as we wish:

Let $s\in \mathbb{R}$. Starting from the beginning of the series, we choose successive positive or zero terms in the series until their partial sum is greater than or equal to $s$. Then we choose successive strictly negative terms, starting again from the beginning of the series, until the partial sum of all the terms is strictly less than $s$. After that, we choose successive positive or zero terms until the partial sum is greater than or equal $s$, followed by negative terms until the partial sum is strictly less than $s$, and so on. The partial sums are greater than $s$ by at most the value of the last positive term retained, and are less than $s$ by at most the value of the last negative term retained. Since $a_n\to 0$, it follows that the rearranged series converges to $s$. 

A similar argument shows that we can rearrange a conditional convergent series to diverge to $\infty$ or $-\infty$, and that we can rearrange the series so that it diverges in a finite or infinite oscillatory fashion.

\subsection{Comparison tests}

\textbf{Theorem(Comparison test):} Suppose that $b_n\geq 0$ and $\sum_{n=1}^{\infty}{b_n}$ converges. If $|a_n|\leq b_n$, then $\sum_{n=1}^{\infty}{a_n}$ converges absolutely. 
On the other hand, if $b_n\geq c_n \geq 0$ and $\sum_{n=1}^{\infty}{c_n}$ diverges, then $\sum_{n=1}^{\infty}{b_n}$ diverges as well.

\textbf{Proof:} $(\sum_{n=1}^{k}{b_n})_k$ is Cauchy, therefore $\sum_{n=m+1}^{k}{|a_n|}\leq \sum_{n=m+1}^{k}{b_n}$ implies that $(\sum_{n=1}^{k}{|a_n|})$ is Cauchy too. 
Similarly, $(\sum_{n=1}^{k}{c_n})_k$ is not Cauchy, therefore $\sum_{n=m+1}^{k}{c_n}\leq \sum_{n=m+1}^{k}{b_n}$ implies that $(\sum_{n=1}^{k}{b_n})_k$ is not Cauchy either, thus $\sum_{n=1}^{\infty}{b_n}$ diverges.

\textbf{Theorem(Limit Comparison test):} Suppose that we have two series $\sum_{n=1}^{\infty}{a_n}$ and $\sum_{n=1}^{\infty}{b_n}$ with $a_n\geq 0, b_n > 0$. If $\lim{n\to \infty}{a_n\over a_b}=c$ exists with $0<c<\infty$, then either both series converge or both diverge.

\textbf{Proof:} Given $\epsilon > 0$ there exist $N\in \mathbb{N}$ s.t. for all $n>N$ we have $x-\epsilon <{a_n\over b_n} < c + \epsilon$ implying $(c-\epsilon)b_n<a_n<(c+\epsilon)b_n$. By choosing $\epsilon>0$ sufficiently small we can assume that $c-\epsilon >0$. Then $(c-\epsilon)b_n<a_n<(c+\epsilon)b_n$ implies using the previous result that either both $(\sum_{n=1}^{k}{b_n})_k$ and $(\sum_{n=1}^{k}{a_n})$ are Cauchy or both of them are not Cauchy.

\subsection{Cauchy condensation test}
\textbf{Theorem (Cauchy Condensation Test).}
For a monotone decreasing sequence \( a_n \ge 0 \), the series 
\[
\sum_{n=1}^{\infty} a_n
\]

converges (or diverges) if and only if the “condensed” series
\[
\sum_{n=0}^{\infty} 2^n a_{2^n}
\]

converges (or diverges).

\textbf{Proof.}

The result follows from the claim:
\[
\sum_{n=1}^{\infty} a_n 
\le 
\sum_{n=0}^{\infty} 2^n a_{2^n}
\le 
2\sum_{n=1}^{\infty} a_n.
\]

\textit{Proof of the claim:}
Since \( a_n \ge a_{n+1} \), we can group the terms as follows:
\[
\sum_{n=1}^{\infty} a_n 
= a_1 + (a_2 + a_3) + (a_4 + a_5 + a_6 + a_7) + \cdots
\]
and since each term is decreasing,
\[
\le a_1 + (a_2 + a_2) + (a_4 + a_4 + a_4 + a_4) + \cdots
= a_1 + 2a_2 + 4a_4 + \cdots
= \sum_{n=0}^{\infty} 2^n a_{2^n}.
\]

On the other hand,
\[
\sum_{n=0}^{\infty} 2^n a_{2^n}
= (a_1 + a_2) + (a_2 + a_4 + a_4 + a_4) + \cdots
\le (a_1 + a_1) + (a_2 + a_2) + (a_3 + a_3) + \cdots
= 2\sum_{n=1}^{\infty} a_n.
\]
Hence the inequalities hold, proving the claim.


\textbf{Example.}

Consider the harmonic series
\[
\sum_{n=1}^{\infty} \frac{1}{n}.
\]
The corresponding condensed series is
\[
\sum_{n=0}^{\infty} 2^n \cdot \frac{1}{2^n} = \sum_{n=0}^{\infty} 1,
\]
which diverges. Therefore, by the Cauchy condensation test, the harmonic series diverges.

\subsection{Geometric series}

Because $\sum_{n=0}^{k}{x^n}={1-x^{k+1}\over 1-x}$ we have

\textbf{Theorem:} $\sum_{n=0}^{k}{x^n}$ converges if $|x|<1$ and diverges if $|x|\geq 1$.

\textbf{Proof:} If $|x|<1$, then $x^k\to 0$, otherwise ${1-x^{k+1}\over 1-x}$ is not Cauchy.

\subsection{Ratio test}


\textbf{Theorem (Ratio Test).}
Suppose that \( a_n \neq 0 \) and let
\[
r = \limsup_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right|.
\]
Then the series
\[
\sum_{n=1}^{\infty} a_n
\]
converges absolutely if \( 0 \le r < 1 \), and diverges if \( 1 < r \le \infty \).

\textbf{Proof.}

If \( r < 1 \), choose \( r < s < 1 \).  
Then there exists \( N \in \mathbb{N} \) such that
\[
\left| \frac{a_{n+1}}{a_n} \right| < s \quad \text{for all } n > N.
\]
It follows that there exists \( M > 0 \) such that
\[
|a_n| \le M s^n \quad \text{for all } n > N.
\]
Therefore,
\[
\sum_{n=m+1}^{k} |a_n| \le \sum_{n=m+1}^{k} M s^n,
\]
which implies that \( \left( \sum_{n=1}^{k} |a_n| \right)_k \) is a Cauchy sequence,  
because the geometric series \( M \sum_{n=0}^{\infty} s^n \) converges.

Similarly, when \( r > 1 \), choose \( r > s > 1 \).  
Then there exists \( N \in \mathbb{N} \) such that
\[
\left| \frac{a_{n+1}}{a_n} \right| > s \quad \text{for all } n > N,
\]
so that
\[
|a_n| \ge M s^n.
\]
It follows that \( a_n \) does not tend to 0,  
so the series \( \sum_{n=1}^{\infty} a_n \) diverges.

When \( r = 1 \), the test is inconclusive.


\subsection{Root test}

\textbf{Theorem (Root Test).}

Suppose that \( a_n \in \mathbb{R} \) and let
\[
r = \limsup_{n \to \infty} |a_n|^{1/n}.
\]
Then the series
\[
\sum_{n=1}^{\infty} a_n
\]
converges absolutely if \( 0 \le r < 1 \), and diverges if \( 1 < r \le \infty \).

\textbf{Proof.}

If \( r < 1 \), choose \( r < s < 1 \).  
Then there exists \( N \in \mathbb{N} \) such that
\[
|a_n|^{1/n} < s \quad \text{for all } n > N.
\]
It follows that
\[
|a_n| < s^n \quad \text{for all } n > N.
\]
Therefore,
\[
\sum_{n=m+1}^{k} |a_n| \le \sum_{n=m+1}^{k} s^n,
\]
which implies that \( \left( \sum_{n=1}^{k} |a_n| \right)_k \) is Cauchy,  
because the geometric series \( \sum_{n=0}^{\infty} s^n \) converges.

Similarly, if \( r > 1 \), choose \( r > s > 1 \).  
Then there exists \( N \in \mathbb{N} \) such that
\[
|a_n|^{1/n} > s \quad \text{for all } n > N,
\]
so that
\[
|a_n| > s^n.
\]
It follows that \( a_n \) does not tend to 0,  
so the series \( \sum_{n=1}^{\infty} a_n \) diverges.

When \( r = 1 \), the test is inconclusive.

\section{Limits of functions}

\begin{itemize}
    \item \textbf{Def(Limit of a function):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. Then 
    \[
    \lim_{x \to c} f(x) = L 
    \]
    if for every $\epsilon > 0$ there exists a $\delta > 0$ such that 
    \[
    0 < |x - c| < \delta \text{ and } x \in A \implies |f(x) - L| < \epsilon.
    \] 
    We also use the notation $f(x) \to L$ as $x \to c$. 
    \item We do not consider what happens when $x = c$, and a function need not be defined at $c$ for its limit to exist. 
    \item \textbf{Corollary:} If $\displaystyle \lim_{x \to c} f(x) = L$ exists, then it is unique by the same argument as for sequences.

    \item \textbf{Theorem (sequential characterization of the limit):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. Then 
    \[
    \displaystyle \lim_{x \to c} f(x) = L
    \]
    if and only if
    \[
    \lim_{n \to \infty} f(x_n) = L
    \]
    for every sequence $(x_n) \in A$ with $x_n \neq c$ and $\displaystyle \lim_{n \to \infty} x_n = c $.
    \item \textbf{Proof:} $\implies$: By the assumption there exists a $\delta > 0$ such that $0 < |x - c| < \delta$ and $x \in A$ implies $|f(x) - L| < \varepsilon$. Since $x_n \to c$, there exists $N \in \mathbb{N}$ s.t. $0 < |x_n - c| < \delta$ for all $n > N$. Then, it follows that $|f(x_n) - L| < \varepsilon$ for all $n > N$.
    
    $\Longleftarrow$: On the contrary assume that there exists $\varepsilon > 0$ s.t. for all $\delta > 0$ there is a point $x \in A$ with
    
    $0 < |x - c| < \delta$ but $|f(x) - L| \geq \varepsilon$. Thus, for every $n \in \mathbb{N}$ there exists an $x_n \in A$ s.t.

    $0 < |x_n - c| < \frac{1}{n}$ and $|f(x_n) - L| \geq \varepsilon$. It follows that $x_n \neq c$ and $\displaystyle \lim_{n \to \infty}x_n = c$, but $\displaystyle \lim_{n \to \infty} f(x_n) \neq L$.
\end{itemize}

\subsection{Left, right and infinite limits}

\begin{itemize}
\item \textbf{Def(left/right limits):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. Then $f$ has the right limit
\[
\lim_{x \to c^+} f(x) = L
\]
if for every $\varepsilon > 0$ there exists a $\delta > 0$ such that $0 < x - c < \delta$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$.

Similarly, $f$ has the left limit
\[
\lim_{x \to c^-} f(x) = L
\]
if for every $\varepsilon > 0$ there exists a $\delta > 0$ such that $0 < c - x < \delta$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$.

\item \textbf{Proposition:} Suppose $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of both $\{x \in A:x > c\}$ and $\{x \in A:x < c\}$. Then $\displaystyle \lim_{x \to c} f(x) = L$ iff $\displaystyle \lim_{x \to c^+} f(x) = \lim_{x \to c^-} f(x) = L$.

\item \textbf{Proof:} Existence of the limit implies the existence of the left and right limits and that they are equal. Conversely $\displaystyle \lim_{x \to c^+} f(x) = \lim_{x \to c^-} f(x) = L$ implies that for all $\varepsilon > 0$ there exists a $\delta_1, \delta_2 > 0$ s.t. $0 < x - c < \delta_1$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$; and $0 < c - x < \delta_2$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$. Choosing $\delta = \min\{\delta_1, \delta_2\}$, we get that $0 < |x - c| < \delta$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$ as claimed.

\item \textbf{Def(limits as $x \to \pm \infty$):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $A$ is not bounded from above. Then
\[
\lim_{x \to \infty} f(x) = L
\]
if for every $\varepsilon > 0$ there exists a $M \in \mathbb{R}$ such that $x > M$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$.

Similarly, when $A$ is not bounded from below, we define
\[
\lim_{x \to -\infty} f(x) = L
\]
if for every $\varepsilon > 0$ there exists a $M \in \mathbb{R}$ such that $x < M$ and $x \in A$ implies that $|f(x) - L| < \varepsilon$.

One can easily see that $\displaystyle \lim_{x \to \infty} f(x) = \lim_{t \to 0^+} f\left(\frac{1}{t}\right)$ and $\displaystyle \lim_{x \to -\infty} f(x) = \lim_{t \to 0^-} f\left(\frac{1}{t}\right) = L$.

\item \textbf{Def(Divergence to $\pm \infty$):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. Then
\[
\lim_{x \to c} f(x) = \infty
\]
if for every $M \in \mathbb{R}$ there exists a $\delta > 0$ such that $0 < |x - c| < \delta$ and $x \in A$ implies that $f(x) > M$.

Similarly,
\[
\lim_{x \to c} f(x) = -\infty
\]
if for every $m \in \mathbb{R}$ there exists a $\delta > 0$ such that $0 < |x - c| < \delta$ and $x \in A$ implies that $f(x) < m$.

The left and right limits in these cases are defined accordingly.
\end{itemize}

\subsection{Properties of limits}

The proof of the following results now follows from corresponding ones proved for sequences.

\begin{itemize}
\item \textbf{Theorem(monotonicity of the limit):} Let $f, g: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. If $f(x) \leq g(x)$ for all $x \in A$, then
\[
\lim_{x \to c} f(x) \leq \lim_{x \to c} g(x)
\]
whenever both limits exist.

\item \textbf{Theorem(Squeeze theorem):} Let $f, g, h: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$. If $f(x) \leq g(x) \leq h(x)$ for all $x \in A$ and $\displaystyle \lim_{x \to c} f(x) = \lim_{x \to c} h(x) = L$, then $\displaystyle \lim_{x \to c} g(x) = L$.

\item \textbf{Theorem(algebraic properties of limits):} Let $f, g: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in \mathbb{R}$ is an accumulation point of $A$ and assume that $\displaystyle \lim_{x \to c} f(x)$, $\displaystyle \lim_{x \to c} g(x) = M$ exist. Then
\begin{enumerate}
    \item $\displaystyle \lim_{x \to c} \alpha f(x) = \alpha \lim_{x \to c}f(x)$ for every $\alpha \in \mathbb{R}$;
    \item $\displaystyle \lim_{x \to c} (f(x) + g(x)) = \lim_{x \to c}f(x) + \lim_{x \to c}g(x)$;
    \item $\displaystyle \lim_{x \to c} (f(x)g(x)) = (\lim_{x \to c}f(x))(\lim_{x \to c}g(x))$;
    \item $\displaystyle \lim_{x \to c} \frac{f(x)}{g(x)} = \frac{\displaystyle \lim_{x \to c}f(x)}{\displaystyle \lim_{x \to c}g(x)}$ when $\displaystyle \lim_{x \to c}g(x) \neq 0$.
\end{enumerate}
\end{itemize}

\section{Continuous functions}

\begin{itemize}
\item \textbf{Def(continuous function):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in A$. Then $f$ is \emph{continuous} at $c \in A$ if
\[
\lim_{x \to c} f(x) = f(c).
\]

\item \textbf{Def(left/right continuity):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and suppose that $c \in A$. Then $f$ is \emph{continuous from the right} at $c \in A$ if
\[
\lim_{x \to c^+} f(x) = f(c).
\]
Similarly, $f$ is \emph{continuous from the left} at $c \in A$ if
\[
\lim_{x \to c^-} f(x) = f(c).
\]

\item \textbf{Theorem:} If $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, and $c \in A$ is an accumulation point of $A$, then $f$ is continuous at $c$ iff
\[
\lim_{n \to \infty} f(x_n) = f(c)
\]
for every sequence $(x_n) \subseteq A$ with $\displaystyle \lim_{n \to \infty} x_n = c$.
\end{itemize}

\subsection{Properties of continuous functions}

Classification of discontinuities of $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$, $c \in A$ is an accumulation point of $A$:
\begin{enumerate}
\item \textbf{Removable discontinuity:} $\displaystyle \lim_{x \to c} f(x) = L$ exists, but $f(c) \neq L$.
\item \textbf{Jump discontinuity:} $\displaystyle \lim_{x \to c} f(x)$ does not exists, but at least $\displaystyle \lim_{x \to c^+} f(x) \neq \lim_{x \to c^-} f(x)$ where both one-sided limits exist.
\item \textbf{Essential discontinuity:} $\displaystyle \lim_{x \to c} f(x)$ does not exists, and at least one of $\displaystyle \lim_{x \to c^+} f(x)$ or $\displaystyle \lim_{x \to c^-} f(x)$ does not exist either.
\end{enumerate}

\begin{itemize}
\item \textbf{Theorem(algebraic properties of continuous functions):} If $f, g: A \to \mathbb{R}$ are continuous at $c \in A$, then $\alpha f(x)$ for any $\alpha \in \mathbb{R}$, $f(x) + g(x)$, $f(x)g(x)$ are all continuous at $c \in A$. Moreover, if $g(c) \neq 0$ then $\frac{f(x)}{g(x)}$ is also continuous at $c \in A$.

\item \textbf{Corollary:} a polynomial function $\displaystyle p(x) = \sum_{i=0}^{n} c_ix^i$ with $c_i \in \mathbb{R}$ is continuous at every $x \in \mathbb{R}$. More generally a rational function $r(x) = \frac{p(x)}{q(x)}$ with $p, q$ polynomials is continuous at every $x \in \mathbb{R}$ where it is defined, that is where $q(x) \neq 0$.

\item \textbf{Theorem:} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$. Then $f$ is continuous on $A$ iff the inverse image $f^{-1}(U) = \{x \in A: f(x) \in U\}$ is open in $A$ for any open set $U \subseteq Ran(f)$.

\item \textbf{Proof:} $\implies$: Let $c \in f^{-1}(U)$. Then $f(c) \in U$ for $U$ open, thus there exists $\varepsilon > 0$ s.t. the open set $U_f(c) = (f(c) - \varepsilon, f(c) + \varepsilon) \subseteq U$. By continuity of $f$ at $c$, there exists $\delta > 0$ s.t. the open set $V_c = (c - \delta, c + \delta)$ satisfies $f(A \cap V_c) \subseteq U_f(c)$. This means that $x \in (c - \delta, c + \delta)$ implies $f(x) \in (f(c) - \epsilon, f(c) + \epsilon)$, thus $f^{-1}(U)$ contains an open neighborhood in $A$.

$\Longleftarrow$: Assume that $f^{-1}(U)$ is open in $A$ for any open $U \subseteq Ran(f)$ and let $c \in U$. Then for small enough $\varepsilon > 0$ the preimage of the open set $(f(c) - \varepsilon, f(c) + \varepsilon)$ is open, so it contains an open set $(c - \delta, c + \delta)$ in $A$. This implies that $|f(x) - f(c)| < \varepsilon$ whenever $|x - c| < \delta$ and $x \in A$, that is, the continuity of $f$ at $c \in U$.

\item \textbf{Theorem:} Let $f: A \to \mathbb{R}$, $g: B \to \mathbb{R}$ where $A, B \subseteq \mathbb{R}$. If $f$ is continuous at $c \in A$ and $g$ is continuous at $f(c) \in B$, then $g \circ f$ is continuous at $c$.

\item \textbf{Proof:} $f$ maps small open neighborhoods $U_c$ of $c \in A$ into small open neighborhoods $V_f(c)$ of $f(c) \in B$ which in turn are also mapped to small open neighborhoods $W_g(f(c))$ of $g(f(c))$ by $g$.

\item \textbf{Corollary:} Let $f: A \to \mathbb{R}$, $g: B \to \mathbb{R}$ where $f(A) \subseteq B$. If $f$ is continuous on $A$ and $g$ is continuous on $f(A)$, then $g \circ f$ is continuous on $A$.

\item \textbf{Terminology:} closed and bounded sets ($of \mathbb{R}$) are also called \emph{compact} sets. Continuous functions preserve this property:

\item \textbf{Theorem:} If $A \subseteq \mathbb{R}$ is compact and $f: A \to \mathbb{R}$ is continuous, then $f(A)$ is also compact.

\item \textbf{Proof:} We need to show that $f(A)$ is closed and bounded. This is the case, if for any arbitrary sequence $(y_n) \subseteq f(A)$ we can always find a convergent subsequence $(y_{n_k})$ with limit in $f(A)$. Let $(x_n) \subseteq A$ be the preimage of the sequence $(y_n)$, that is $f(x_n) = y_n$. By the Bolzano-Weierstrass theorem there exists a convergent subsequence $(x_{n_k})$ with limit $x \in A$. Then continuity of $f$ implies that $\displaystyle \lim_{k \to \infty}f(x_{n_k}) = f(x) \in f(A)$. This implies that $f(A)$ is closed, and also implies that it contains no unbounded sequence, thus also bounded.
\end{itemize}

\subsection{Extreme value theorem}

\begin{itemize}
\item \textbf{Theorem(Weierstrass' Extreme value theorem):} If $A \subseteq \mathbb{R}$ is compact and $f: A \to \mathbb{R}$ is continuous, then $f$ attains its maximum and minimum on $A$.

\item \textbf{Proof:} $f(A)$ is closed and bounded. Boundedness implies that there exist $\sup f(A)$ and $\inf f(A)$, and then closedness implies that they are included in $f(A)$ and thus have preimages in $A$.
\end{itemize}

\subsection{Intermediate value theorem}

\begin{itemize}
\item \textbf{Theorem(Intermediate value):} Suppose that $f: [a, b] \to \mathbb{R}$ is continuous on the closed and bounded $[a, b]$. If $f(a)$ and $f(b)$ have opposite signs, then there exists $c \in (a, b)$ s.t. $f(c) = 0$.

\item \textbf{Proof:} Suppose that $f(a) < 0$ and $f(b) > 0$, otherwise we can consider $-f$ instead of $f$. The set $A = \{x \in [a, b]: f(x) < 0\}$ is nonempty since $f(a) < 0$, and is bounded from above by $b$. Let $c = \sup A$. We claim that $f(c) = 0$. So, suppose that $f(c) \neq 0$. Then by the continuity of $f$ at $c$, there exists $\delta > 0$ s.t. $x \in [a, b]$ and $|x - c| < \delta$ implies that $|f(x) - f(c)| < \frac{|f(c)|}{2}$. If $f(c) < 0$, then $c \neq b$ and $f(x) = f(c) + f(x) - f(c) < f(c) - \frac{|f(c)|}{2}$ for all $x \in [a, b]$ s.t. $|x - c| < \delta$, so $f(x) < \frac{f(c)}{2} < 0$. It follows that there exists $x \in A$ for which $x > c$, which contradicts that $c = \sup A$. In the remaining case $f(c) > 0$, then $c \neq a$ and $f(x) = f(c) + f(x) - f(c) > f(c) - \frac{f(c)}{2}$ for all $x \in [a, b]$ s.t. $|x - c| < \delta$, so $f(x) > \frac{f(c)}{2} > 0$. It follows that there exists $\eta > 0$ s.t. $c - \eta \geq a$ and $f(x) > 0$ for $c - \eta \leq x \leq c$. Thus $c - \eta$ is a smaller upper bound for $A$ then $c$ which contradicts that $c = \sup A$. This proves $f(c) = 0$ and thus also $c \neq a,b$ since $f(a) < 0$ and $f(b) > 0$.

\item \textbf{Theorem(Intermediate value theorem):} Suppose that $f: [a, b] \to \mathbb{R}$ is continuous on the closed and bounded $[a, b]$. Then for every $d \in (f(a),f(b))$ there is a point $c \in [a, b]$ s.t. $f(c) = d$.
\end{itemize}

\subsection{Uniform continuity}

\begin{itemize}
\item \textbf{Def(uniformly continuous function):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$. Then $f$ is \emph{uniformly continuous on} $A$ if for every $\varepsilon > 0$ there exists a $\delta > 0$ s.t. $x, y \in A$ and $|x - y| < \delta$ implies that $|f(x) - f(y)| < \varepsilon$.

The key point is that $\delta$ depends only on $\varepsilon$, not on $x,y \in A$. In other words, the difference $|f(x) - f(y)|$ is \emph{uniformly} small over the set $A$ given that the difference $|x - y|$ is small.

\item \textbf{Ex.:} Define $f:[0,1] \to \mathbb{R}$ by $f(x) = x^2$. Then $f$ is uniformly continuous on $[0,1]$ because $|f(x) - f(y)| = |x^2 - y^2| = |x + y||x - y| \leq 2|x - y|$, thus we can take $\delta = \frac{\varepsilon}{2}$ on $[0,1]$. Define $g: (0,1] \to \mathbb{R}$ by $g(x) = \frac{1}{x}$. Then $|g(x) - g(y)| = \left|\frac{1}{x} - \frac{1}{y}\right| = \frac{|x - y|}{|xy|}$, thus $g$ is not uniformly continuous on $(0,1]$.

\item \textbf{Def(Lipschitz continuous function):} Let $f: A \to \mathbb{R}$, where $A \subseteq \mathbb{R}$. Then $f$ is \emph{(L-)Lipschitz continuous on} $A$ if there exists $L > 0$ s.t. $x, y \in A$ implies that $|f(x) - f(y)| < L|x - y|$.

\item \textbf{Theorem(Heine's theorem):} If $A \subseteq \mathbb{R}$ is compact and $f: A \to \mathbb{R}$ is continuous, then $f$ is uniformly continuous on $A$.

\item \textbf{Proof:} Suppose on the contrary that $f$ is not uniformly continuous on $A$. Then there exist $\varepsilon > 0$ and sequences $(x_n), (y_n) \subseteq A$ s.t. $|x_n - y_n| \to 0$ and $|f(x_n) - f(y_n)| \geq \varepsilon$ for every $n \in \mathbb{N}$. Since $A$ is closed and bounded by Bolzano-Weierstrass there exist convergent subsequences $(x_{n_k})$ and $(y_{n_k})$ of $(x_n)$ and $(y_n)$ respectively with a common limit $x \in A$ due to $|x_n - y_n| \to 0$. By the continuity of $f$, we get that $|f(x_{n_k}) - f(y_{n_k})| \leq |f(x_{n_k}) - f(x)| + |f(x) - f(y_{n_k})| \to 0$ contradicting $|f(x_{n_k}) - f(y_{n_k})| \geq \varepsilon$.
\end{itemize}

\subsection{Monotonic functions}

\begin{itemize}
\item \textbf{Def(monotone function):} Let $I \subseteq \mathbb{R}$ be an interval. A function $f: I \to \mathbb{R}$ is
\begin{description}
    \item[Increasing] if $f(x_1) \leq f(x_2)$ for $x_1 < x_2$ in $I$, \emph{strictly} if $f(x_1) < f(x_2)$;
    \item[Decreasing] if $f(x_1) \geq f(x_2)$ for $x_1 < x_2$ in $I$, \emph{strictly} if $f(x_1) > f(x_2)$.
\end{description}

\item \textbf{Theorem:} If $f: I \to \mathbb{R}$ is monotone, then $\displaystyle \lim_{x \to c^+} f(x)$ and $\displaystyle \lim_{x \to c^-} f(x)$ exist at every interior point $c \in I$.

\item \textbf{Proof:} Assume that $f$ is increasing, otherwise we can choose $-f$. We claim that $\displaystyle \lim_{x \to c^-} f(x) = \sup\{f(x) \in \mathbb{R}: x \in I, x < c\}$. First, the set $E = \{f(x) \in \mathbb{R}: x \in I, x < c\}$ is not empty since $c \in I$ is an interior point, and $E \leq f(c)$ because $f$ is increasing, so $\sup E$ exists. For $\varepsilon > 0$, there exists $y_0 \in E$ s.t. $\sup E - \epsilon < y_0 \leq \sup E$ and a corresponding $c > x_0 \in I$ with $y_0 = f(x_0)$. Let $\delta = c - x_0$. If $c - \delta < x < c$, then $x_0 < x < c$ implying $f(x_0) \leq f(x) \leq \sup E$. It follows that $\sup E - \epsilon < f(x) \leq \sup E$ when $c - \delta < x < c$, thus $\displaystyle \lim_{x \to c^-} f(x) = \sup E$. A similar argument implies that $\displaystyle \lim_{x \to c^+} f(x)$ exists as well.

\item \textbf{Corollary:} If $f: I \to \mathbb{R}$ is monotone, then at every interior point $c \in I$, $f$ is either continuous or has a jump.

\item \textbf{Proof:} For $f$ increasing $\displaystyle \lim_{x \to c^-} f(x) \leq f(c) \leq \lim_{x \to c^+} f(x)$ holds and implies either $\displaystyle \lim_{x \to c^-} f(x) = \lim_{x \to c^+} f(x)$ or $\displaystyle \lim_{x \to c^-} f(x) \leq f(c) \leq \lim_{x \to c^+} f(x)$ with at least one inequality strict.
\end{itemize}

\subsection{Inverse of a strictly monotone function}

\begin{itemize}
\item \textbf{Proposition(inverse of a strictly monotone function):} Let $f: I \to \mathbb{R}$ be a strictly monotone and continuous function on an interval $I$. Then $f^{-1}: f(I) \to I$ exists, $f(I)$ is an interval, moreover $f^{-1}$ is also strictly monotone and continuous.

\item \textbf{Proof:} Assume that $f$ is strictly increasing, the case of decreasing $f$ has similar proof. Then $f$ is injective, because $x, y \in I$ and $x \neq y$ implies that either $x < y$ or $x > y$, which implies by that either $f(x) < f(y)$ or $f(x) > f(y)$, since $f$ is strictly increasing. Thus $f^{-1}: f(I) \to I$ exists, moreover $f(x) < f(y)$ implies $x < y$, since $f(x) < f(y)$ and $x \geq y$ would contradict that $f$ is increasing. The intermediate value theorem implies that $f(I)$ is an interval, because $I$ is also. Now to see that $f^{-1}$ is continuous, it is enough to see that $f^{-1}$ is continuous on every closed and bounded interval $[f(a),f(b)]$ for $a < b$. Let $(y_n),y \in [f(a),f(b)]$ be such that $y_n \to y$, that is $f(x_n) = y_n \to y = f(x)$ for corresponding $x_n, x \in [a,b]$. What we need to show that $x_n \to x$ as well. Since $[a,b]$ is bounded there exist $\displaystyle \limsup_{n \to \infty} x_n$ and $\displaystyle \liminf_{n \to \infty} x_n$ with corresponding subsequences $(x_{n_j}), (x_{n_k})$ s.t. $\displaystyle \lim_{j \to \infty}x_{n_j} = \limsup_{n \to \infty}x_n$ and $\displaystyle \lim_{k \to \infty}x_{n_k} = \liminf_{n \to \infty}x_n$. Then the continuity of $f$ implies $\displaystyle \lim_{j \to \infty}f(x_{n_j}) = f(\limsup_{n \to \infty}x_n)$ and $\displaystyle \lim_{k \to \infty}f(x_{n_k}) = f(\liminf_{n \to \infty}x_n)$ which combined with $f(x_n) \to f(x)$ yields $\displaystyle f(x) = f(\liminf_{n \to \infty}x_n) = f(\limsup_{n \to \infty}x_n)$. Now $f$ is injective, so $\displaystyle \limsup_{n \to \infty}x_n = \liminf_{n \to \infty}x_n = x$, thus $x_n \to x$.
\end{itemize}
    
\end{document}